"""
Direct API test - Compare ai_feedback.py approach vs integration example approach
"""
import streamlit as st
from initialize import init_openai_client

st.title("ğŸ§ª API ç›´æ¥ãƒ†ã‚¹ãƒˆ")

# Initialize client
if "openai_client" not in st.session_state:
    st.session_state.openai_client = init_openai_client()

client = st.session_state.openai_client

if client:
    st.success("âœ… OpenAI client initialized successfully")
    
    # Show client info
    st.write("**Client Info:**")
    st.write(f"- Endpoint: {client._base_url}")
    st.write(f"- API Version: {client._custom_query.get('api-version', 'N/A') if hasattr(client, '_custom_query') else 'N/A'}")
    
    st.divider()
    
    # Test 1: Simple call with gpt-5-mini (like ai_feedback.py)
    st.subheader("ãƒ†ã‚¹ãƒˆ 1: ai_feedback.py ã¨åŒã˜æ–¹æ³•")
    if st.button("ãƒ†ã‚¹ãƒˆ 1 å®Ÿè¡Œ"):
        try:
            with st.spinner("API å‘¼ã³å‡ºã—ä¸­..."):
                response = client.chat.completions.create(
                    model="gpt-5-mini",  # Same as ai_feedback.py
                    messages=[
                        {"role": "system", "content": "ã‚ãªãŸã¯ç™ºéŸ³ã®å°‚é–€å®¶ã§ã™ã€‚"},
                        {"role": "user", "content": "ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã™ã€‚ã“ã‚“ã«ã¡ã¯ï¼"},
                    ],
                    stream=True,
                )
                st.write("**Response (streaming):**")
                st.write_stream(response)
                st.success("âœ… ãƒ†ã‚¹ãƒˆ 1 æˆåŠŸï¼")
        except Exception as e:
            st.error(f"âŒ ãƒ†ã‚¹ãƒˆ 1 å¤±æ•—: {type(e).__name__}: {str(e)}")
    
    st.divider()
    
    # Test 2: Non-streaming call (like integration example)
    st.subheader("ãƒ†ã‚¹ãƒˆ 2: çµ±åˆä¾‹ã¨åŒã˜æ–¹æ³•ï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰")
    if st.button("ãƒ†ã‚¹ãƒˆ 2 å®Ÿè¡Œ"):
        try:
            with st.spinner("API å‘¼ã³å‡ºã—ä¸­..."):
                # Note: gpt-5-mini is a reasoning model, needs more tokens for reasoning + output
                response = client.chat.completions.create(
                    model="gpt-5-mini",
                    messages=[
                        {"role": "system", "content": "ã‚ãªãŸã¯ç™ºéŸ³ã®å°‚é–€å®¶ã§ã™ã€‚"},
                        {"role": "user", "content": "ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã™ã€‚ã“ã‚“ã«ã¡ã¯ï¼"},
                    ],
                    max_completion_tokens=2000  # Increased for reasoning model
                )
                
                # Debug: Show full response object
                st.write("**Debug Info:**")
                st.write(f"- Response type: {type(response)}")
                st.write(f"- Choices count: {len(response.choices)}")
                st.write(f"- First choice: {response.choices[0]}")
                st.write(f"- Message: {response.choices[0].message}")
                st.write(f"- Content type: {type(response.choices[0].message.content)}")
                st.write(f"- Content length: {len(response.choices[0].message.content) if response.choices[0].message.content else 0}")
                
                st.divider()
                st.write("**Response (non-streaming):**")
                content = response.choices[0].message.content
                if content:
                    st.write(content)
                    st.success("âœ… ãƒ†ã‚¹ãƒˆ 2 æˆåŠŸï¼")
                else:
                    st.warning("âš ï¸ Response is empty!")
                    st.json(response.model_dump())
        except Exception as e:
            st.error(f"âŒ ãƒ†ã‚¹ãƒˆ 2 å¤±æ•—: {type(e).__name__}: {str(e)}")
    
    st.divider()
    
    # Test 3: Try with other common deployment names
    st.subheader("ãƒ†ã‚¹ãƒˆ 3: ä»–ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåã‚’è©¦ã™")
    deployment_name = st.text_input("ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå", value="gpt-5-mini")
    if st.button("ãƒ†ã‚¹ãƒˆ 3 å®Ÿè¡Œ"):
        try:
            with st.spinner(f"'{deployment_name}' ã§ API å‘¼ã³å‡ºã—ä¸­..."):
                # Note: reasoning models need higher token limits
                response = client.chat.completions.create(
                    model=deployment_name,
                    messages=[
                        {"role": "system", "content": "ã‚ãªãŸã¯ç™ºéŸ³ã®å°‚é–€å®¶ã§ã™ã€‚"},
                        {"role": "user", "content": "ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã™ã€‚ã“ã‚“ã«ã¡ã¯ï¼"},
                    ],
                    max_completion_tokens=1500
                )
                st.write(f"**Response with '{deployment_name}':**")
                st.write(response.choices[0].message.content)
                st.success(f"âœ… ãƒ†ã‚¹ãƒˆ 3 æˆåŠŸï¼ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå '{deployment_name}' ã¯å‹•ä½œã—ã¾ã™ã€‚")
        except Exception as e:
            st.error(f"âŒ ãƒ†ã‚¹ãƒˆ 3 å¤±æ•—: {type(e).__name__}: {str(e)}")
            st.info(f"ğŸ’¡ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå '{deployment_name}' ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
    
    st.divider()
    
    # Test 4: Check secrets configuration
    st.subheader("ãƒ†ã‚¹ãƒˆ 4: secrets.toml è¨­å®šç¢ºèª")
    try:
        st.write("**AzureGPT Secrets:**")
        azure_gpt = st.secrets.get("AzureGPT", {})
        st.write(f"- AZURE_OPENAI_ENDPOINT: {azure_gpt.get('AZURE_OPENAI_ENDPOINT', 'Not set')}")
        st.write(f"- AZURE_OPENAI_API_KEY: {'***' + azure_gpt.get('AZURE_OPENAI_API_KEY', '')[-4:] if azure_gpt.get('AZURE_OPENAI_API_KEY') else 'Not set'}")
        st.write(f"- DEPLOYMENT_NAME: {azure_gpt.get('DEPLOYMENT_NAME', 'Not set (using default)')}")
    except Exception as e:
        st.error(f"Error reading secrets: {e}")

else:
    st.error("âŒ OpenAI client initialization failed")

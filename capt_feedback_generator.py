"""
Feedback Generator for CAPT feedback pipeline.

This module generates concise, actionable feedback by combining:
1. Guidance Card (stable reference)
2. Attempt Summary (incremental changes)
3. LLM prompting (with token optimization)

The goal is to produce short, consistent, encouraging feedback that focuses
on the most important pronunciation issues without overwhelming the learner.
"""

from typing import Optional, Callable
import json

from capt_models import (
    GuidanceCard,
    AttemptSummary,
    FeedbackPrompt,
)
from capt_config import FeedbackConfig, DEFAULT_CONFIG


def generate_feedback(
    guidance_card: GuidanceCard,
    attempt_summary: AttemptSummary,
    llm_function: Optional[Callable[[str], str]] = None,
    config: Optional[FeedbackConfig] = None
) -> str:
    """
    Generate concise feedback for a pronunciation attempt.
    
    This function:
    1. Creates a compact prompt combining guidance highlights + attempt delta
    2. Calls LLM with optimized prompt
    3. Returns short, actionable feedback
    
    Args:
        guidance_card: Stable reference guidance card
        attempt_summary: Current attempt analysis
        llm_function: Function to call LLM (takes prompt str, returns response str)
                      If None, returns structured prompt without LLM call
        config: Configuration object (uses DEFAULT_CONFIG if None)
        
    Returns:
        Generated feedback string (or prompt if llm_function is None)
        
    Example:
        >>> def my_llm(prompt: str) -> str:
        ...     return openai_client.chat.completions.create(...)
        >>> feedback = generate_feedback(guidance, attempt, my_llm)
        >>> print(feedback)
    """
    if config is None:
        config = DEFAULT_CONFIG
    
    # Build compact guidance highlights
    guidance_highlights = _build_guidance_highlights(guidance_card, config)
    
    # Build compact attempt summary
    attempt_text = _build_attempt_text(attempt_summary, config)
    
    # Create feedback prompt
    prompt = FeedbackPrompt(
        target_text=guidance_card.target_display,
        guidance_highlights=guidance_highlights,
        attempt_summary=attempt_text,
        language=config.feedback_language,
        max_tokens=config.feedback_max_tokens,
        temperature=config.feedback_temperature,
    )
    
    formatted_prompt = prompt.format_prompt()
    
    # If no LLM function provided, return the prompt itself
    if llm_function is None:
        return formatted_prompt
    
    # Call LLM
    try:
        feedback = llm_function(formatted_prompt)
        return feedback
    except Exception as e:
        # Fallback to rule-based feedback if LLM fails
        return _generate_fallback_feedback(attempt_summary, config)


def _build_guidance_highlights(
    guidance: GuidanceCard,
    config: FeedbackConfig
) -> str:
    """
    Build compact guidance highlights (â‰ˆ50-100 tokens).
    
    Focus on the most challenging elements from the guidance card.
    """
    lines = []
    
    # Add challenging phonemes (top 3)
    if guidance.challenging_phonemes:
        phonemes = guidance.challenging_phonemes[:3]
        phoneme_str = ", ".join([
            f"/{p.phoneme}/ in '{p.word}' ({p.score:.0f})" 
            for p in phonemes
        ])
        lines.append(f"é›£ã—ã„éŸ³ç´ : {phoneme_str}")
    
    # Add challenging words (top 2)
    if guidance.challenging_words:
        words = guidance.challenging_words[:2]
        word_str = ", ".join([
            f"'{w.word}' ({w.score:.0f})" if w.score else f"'{w.word}' (çœç•¥)"
            for w in words
        ])
        lines.append(f"é›£ã—ã„å˜èª: {word_str}")
    
    # Add prosody pattern (top 1)
    if guidance.prosody_patterns:
        prosody = guidance.prosody_patterns[0]
        lines.append(f"ãƒªã‚ºãƒ : {prosody.description}")
    
    # Add reference score
    lines.append(f"åˆå›ã‚¹ã‚³ã‚¢: {guidance.reference_overall:.0f}ç‚¹")
    
    return "\n".join(lines)


def _build_attempt_text(
    attempt: AttemptSummary,
    config: FeedbackConfig
) -> str:
    """
    Build compact attempt summary (â‰ˆ50-100 tokens).
    
    Focus on:
    - Current score
    - Most critical errors
    - Improvements (if any)
    """
    lines = []
    
    # Current score with category
    category = config.classify_score(attempt.overall_score)
    category_label = config.score_labels.get(category, "")
    lines.append(
        f"ç¬¬{attempt.attempt_number}å›: {attempt.overall_score:.0f}ç‚¹ ({category_label})"
    )
    
    # Score breakdown
    lines.append(
        f"æ­£ç¢ºæ€§: {attempt.accuracy_score:.0f}, "
        f"æµæš¢æ€§: {attempt.fluency_score:.0f}, "
        f"éŸ»å¾‹: {attempt.prosody_score:.0f}"
    )
    
    # Critical errors (top 2)
    if attempt.current_word_errors:
        errors = attempt.current_word_errors[:2]
        error_str = ", ".join([
            f"'{e.word}' ({e.error_type.value})" 
            for e in errors
        ])
        lines.append(f"ã‚¨ãƒ©ãƒ¼: {error_str}")
    
    if attempt.current_phoneme_errors:
        errors = attempt.current_phoneme_errors[:2]
        phoneme_str = ", ".join([
            f"/{e.phoneme}/ in '{e.word}'" 
            for e in errors
        ])
        lines.append(f"éŸ³ç´ ã‚¨ãƒ©ãƒ¼: {phoneme_str}")
    
    # Improvements
    if attempt.improved_words:
        improved_str = ", ".join(attempt.improved_words[:2])
        lines.append(f"âœ… æ”¹å–„: {improved_str}")
    
    # Omissions
    if attempt.omitted_words:
        omitted_str = ", ".join(attempt.omitted_words[:2])
        lines.append(f"âš ï¸ çœç•¥: {omitted_str}")
    
    return "\n".join(lines)


def _generate_fallback_feedback(
    attempt: AttemptSummary,
    config: FeedbackConfig
) -> str:
    """
    Generate rule-based feedback if LLM is unavailable.
    
    Simple template-based feedback focusing on scores and main issues.
    """
    category = config.classify_score(attempt.overall_score)
    
    # Start with score assessment
    if category == "excellent":
        feedback = f"ç´ æ™´ã‚‰ã—ã„ç™ºéŸ³ã§ã™ï¼ã‚¹ã‚³ã‚¢: {attempt.overall_score:.0f}ç‚¹\n"
    elif category == "good":
        feedback = f"è‰¯ã„ç™ºéŸ³ã§ã™ã€‚ã‚¹ã‚³ã‚¢: {attempt.overall_score:.0f}ç‚¹\n"
    elif category == "fair":
        feedback = f"ã¾ãšã¾ãšã®ç™ºéŸ³ã§ã™ã€‚ã‚¹ã‚³ã‚¢: {attempt.overall_score:.0f}ç‚¹\n"
    else:
        feedback = f"ã‚‚ã†å°‘ã—ç·´ç¿’ã—ã¾ã—ã‚‡ã†ã€‚ã‚¹ã‚³ã‚¢: {attempt.overall_score:.0f}ç‚¹\n"
    
    # Add main issue
    if attempt.current_word_errors:
        word = attempt.current_word_errors[0]
        feedback += f"'{word.word}'ã®ç™ºéŸ³ã«æ³¨ç›®ã—ã¦ãã ã•ã„ã€‚\n"
    elif attempt.current_phoneme_errors:
        phoneme = attempt.current_phoneme_errors[0]
        feedback += f"/{phoneme.phoneme}/ã®éŸ³ã«æ³¨æ„ã—ã¾ã—ã‚‡ã†ã€‚\n"
    
    # Add improvement note
    if attempt.improved_words:
        feedback += f"'{attempt.improved_words[0]}'ãŒæ”¹å–„ã•ã‚Œã¾ã—ãŸï¼\n"
    
    # Encouragement
    if attempt.attempt_number > 1:
        feedback += "ç¶™ç¶šã—ã¦ç·´ç¿’ã™ã‚‹ã“ã¨ã§ã€ã•ã‚‰ã«ä¸Šé”ã—ã¾ã™ï¼"
    else:
        feedback += "æ¬¡å›ã¯ã‚‚ã£ã¨è‰¯ããªã‚‹ã¯ãšã§ã™ï¼"
    
    return feedback


def generate_structured_feedback(
    guidance_card: GuidanceCard,
    attempt_summary: AttemptSummary,
    config: Optional[FeedbackConfig] = None
) -> dict:
    """
    Generate structured feedback data (without LLM).
    
    Returns a dictionary with organized feedback components for UI display.
    
    Args:
        guidance_card: Stable reference guidance card
        attempt_summary: Current attempt analysis
        config: Configuration object (uses DEFAULT_CONFIG if None)
        
    Returns:
        Dictionary with feedback components:
        - overall_score: Overall pronunciation score
        - score_category: Category label (excellent, good, fair, poor)
        - main_issues: List of main pronunciation issues
        - improvements: List of improvements from previous attempts
        - recommendations: List of actionable recommendations
        - encouragement: Encouraging message
    """
    if config is None:
        config = DEFAULT_CONFIG
    
    category = config.classify_score(attempt_summary.overall_score)
    
    # Identify main issues (top 3)
    main_issues = []
    for word_error in attempt_summary.current_word_errors[:3]:
        if word_error.error_type.value == "Omission":
            main_issues.append(f"'{word_error.word}'ãŒçœç•¥ã•ã‚Œã¾ã—ãŸ")
        else:
            score_str = f"{word_error.score:.0f}ç‚¹" if word_error.score else "N/A"
            main_issues.append(f"'{word_error.word}'ã®ç™ºéŸ³ ({score_str})")
    
    for phoneme_error in attempt_summary.current_phoneme_errors[:2]:
        main_issues.append(
            f"/{phoneme_error.phoneme}/ã®éŸ³ in '{phoneme_error.word}' ({phoneme_error.score:.0f}ç‚¹)"
        )
    
    # Identify improvements
    improvements = []
    if attempt_summary.improved_words:
        improvements.extend([
            f"'{word}'ãŒæ”¹å–„ã•ã‚Œã¾ã—ãŸ" for word in attempt_summary.improved_words[:2]
        ])
    if attempt_summary.improved_phonemes:
        improvements.extend([
            f"{phoneme}ãŒæ”¹å–„ã•ã‚Œã¾ã—ãŸ" for phoneme in attempt_summary.improved_phonemes[:2]
        ])
    
    # Generate recommendations
    recommendations = []
    if attempt_summary.current_phoneme_errors:
        top_phoneme = attempt_summary.current_phoneme_errors[0]
        recommendations.append(
            f"/{top_phoneme.phoneme}/ã®ç™ºéŸ³ã‚’é‡ç‚¹çš„ã«ç·´ç¿’ã—ã¾ã—ã‚‡ã†"
        )
    if attempt_summary.current_prosody_issues:
        top_prosody = attempt_summary.current_prosody_issues[0]
        recommendations.append(top_prosody.description)
    if attempt_summary.fluency_score < config.fluency_min_acceptable:
        recommendations.append("ã‚‚ã†å°‘ã—ã‚†ã£ãã‚Šã€ã¯ã£ãã‚Šã¨è©±ã—ã¦ã¿ã¾ã—ã‚‡ã†")
    
    # Generate encouragement
    if category == "excellent":
        encouragement = "ç´ æ™´ã‚‰ã—ã„ç™ºéŸ³ã§ã™ï¼ã“ã®èª¿å­ã§ç¶šã‘ã¦ãã ã•ã„ã€‚"
    elif category == "good":
        encouragement = "è‰¯ã„ç™ºéŸ³ã§ã™ã€‚ã‚ã¨å°‘ã—ã§ãƒ‘ãƒ¼ãƒ•ã‚§ã‚¯ãƒˆã§ã™ï¼"
    elif category == "fair":
        encouragement = "ç€å®Ÿã«é€²æ­©ã—ã¦ã„ã¾ã™ã€‚ç·´ç¿’ã‚’ç¶šã‘ã¾ã—ã‚‡ã†ã€‚"
    else:
        encouragement = "è«¦ã‚ãšã«ç·´ç¿’ã™ã‚Œã°ã€å¿…ãšä¸Šé”ã—ã¾ã™ï¼"
    
    return {
        "overall_score": attempt_summary.overall_score,
        "score_category": category,
        "score_label": config.score_labels.get(category, ""),
        "main_issues": main_issues,
        "improvements": improvements,
        "recommendations": recommendations,
        "encouragement": encouragement,
        "attempt_number": attempt_summary.attempt_number,
    }


def format_feedback_for_display(structured_feedback: dict) -> str:
    """
    Format structured feedback into a readable string for display.
    
    Args:
        structured_feedback: Dictionary from generate_structured_feedback()
        
    Returns:
        Formatted feedback string
    """
    lines = []
    
    # Header with score
    lines.append(f"ğŸ“Š ç·åˆã‚¹ã‚³ã‚¢: {structured_feedback['overall_score']:.0f}ç‚¹")
    lines.append(f"   è©•ä¾¡: {structured_feedback['score_label']}")
    lines.append("")
    
    # Main issues
    if structured_feedback['main_issues']:
        lines.append("ğŸ¯ ä¸»ãªèª²é¡Œ:")
        for issue in structured_feedback['main_issues']:
            lines.append(f"   â€¢ {issue}")
        lines.append("")
    
    # Improvements
    if structured_feedback['improvements']:
        lines.append("âœ… æ”¹å–„ç‚¹:")
        for improvement in structured_feedback['improvements']:
            lines.append(f"   â€¢ {improvement}")
        lines.append("")
    
    # Recommendations
    if structured_feedback['recommendations']:
        lines.append("ğŸ’¡ ã‚¢ãƒ‰ãƒã‚¤ã‚¹:")
        for rec in structured_feedback['recommendations']:
            lines.append(f"   â€¢ {rec}")
        lines.append("")
    
    # Encouragement
    lines.append(f"ğŸ’ª {structured_feedback['encouragement']}")
    
    return "\n".join(lines)

"""
Complete integration example for CAPT feedback in PhonoEcho.
This shows a working implementation you can adapt to your needs.

Usage: Copy relevant sections into your phonoecho.py
"""

import streamlit as st
from streamlit_advanced_audio import audix, CustomizedRegion, RegionColorOptions
from initialize import reset_page_padding, initialize_session_state, init_openai_client
from data_loader import load_video, load_text
from ai_feedback import get_ai_feedback, get_pronunciation_assessment, save_pronunciation_assessment
from audio_process import save_audio_to_file
from chart import create_radar_chart

# NEW: Import CAPT modules
from capt_integration import CAPTFeedbackPipeline
from capt_config import FeedbackConfig

reset_page_padding()

with st.sidebar:
    st.title("PhonoEcho")
    user = st.number_input("ãƒ¦ãƒ¼ã‚¶ãƒ¼ç•ªå·", min_value=1, max_value=24, value=1, step=1)
    lesson = st.number_input("ãƒ¬ãƒƒã‚¹ãƒ³", min_value=1, max_value=4, value=1, step=1)


# Initialize session state (modify initialize.py to include CAPT pipeline)
initialize_session_state(st.session_state, user, lesson)

# NEW: Initialize CAPT pipeline if not exists
if "capt_pipeline" not in st.session_state:
    config = FeedbackConfig(
        fair_threshold=60.0,  # Minimum score for "fair" rating
        phoneme_error_threshold=70.0,  # Phoneme scores below this are errors
        word_error_threshold=70.0,  # Word scores below this are problematic
        prosody_error_threshold=65.0,  # Prosody scores below this indicate issues
        max_attempt_errors=5,  # Maximum errors to report per attempt
    )
    st.session_state.capt_pipeline = CAPTFeedbackPipeline(
        user_id=user,
        lesson_id=lesson,
        config=config
    )

if "guidance_card" not in st.session_state:
    st.session_state.guidance_card = None

if "ai_feedback_text" not in st.session_state:
    st.session_state.ai_feedback_text = None


def generate_ai_feedback_from_structured(client, structured_feedback, reference_text):
    """
    Generate AI feedback using structured CAPT feedback data.
    
    Args:
        client: OpenAI client instance
        structured_feedback: Dictionary from get_structured_feedback()
        reference_text: The reference text for the lesson
    
    Returns:
        str: Generated feedback text in Japanese
    """
    system_prompt = """ã‚ãªãŸã¯å­¦ç¿’è€…ã®å°‚å±ç™ºéŸ³ãƒãƒ¥ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚æ¸©ã‹ãã€è¦ªã—ã¿ã‚„ã™ãã€åŠ±ã¾ã—ãªãŒã‚‰æŒ‡å°ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ç‚¹ã‚’å«ã‚ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã—ã¦ãã ã•ã„ï¼š
1. ğŸ¯ å‰å‘ãã§å…·ä½“çš„ãªè©•ä¾¡ã‹ã‚‰å§‹ã‚ã‚‹
2. ğŸ’¡ ãƒŸã‚¹ãŒã‚ã‚‹å ´åˆï¼šãªãœãã®ãƒŸã‚¹ãŒèµ·ããŸã®ã‹ã€ä¼šè©±èª¿ã§åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜
   ã€€ ãƒŸã‚¹ãŒãªã„å ´åˆï¼šä»Šå›ã®ç™ºéŸ³ã®è‰¯ã‹ã£ãŸç‚¹ã‚’å…·ä½“çš„ã«è¤’ã‚ã€éŸ»å¾‹é¢ã§ã®æ”¹å–„ç‚¹ãŒã‚ã‚Œã°å„ªã—ãæŒ‡æ‘˜
3. ğŸ—£ï¸ ã‚·ãƒ³ãƒ—ãƒ«ãªè¨€è‘‰ã§ã€å®Ÿä¾‹ã‚’ä½¿ã£ã¦èª¬æ˜
4. âœ¨ å…·ä½“çš„ã§å®Ÿè·µã—ã‚„ã™ã„ç·´ç¿’æ–¹æ³•ã‚’2ã€œ3å€‹ææ¡ˆ
5. ğŸŒŸ æ¬¡å›ã«å‘ã‘ãŸåŠ±ã¾ã—ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§çµ‚ã‚ã‚‹

é‡è¦ãªãƒ«ãƒ¼ãƒ«ï¼š
- ç™ºéŸ³èª¬æ˜ã¯å¿…ãšIPAï¼ˆå›½éš›éŸ³å£°è¨˜å·ï¼‰ã‚’ä½¿ç”¨
- ã‚«ã‚¿ã‚«ãƒŠã‚„ä»®åã¯ä½¿ã‚ãªã„
- ã¾ã‚‹ã§éš£ã«ã„ã‚‹ã‹ã®ã‚ˆã†ãªã€ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§è¦ªã—ã¿ã‚„ã™ã„å£èª¿
- æ—¥æœ¬èªã§å›ç­”"""
    
    # Extract data from structured feedback (correct keys)
    overall_score = structured_feedback.get("overall_score", 0)
    score_category = structured_feedback.get("score_category", "fair")
    score_label = structured_feedback.get("score_label", "")
    main_issues = structured_feedback.get("main_issues", [])
    improvements = structured_feedback.get("improvements", [])
    recommendations = structured_feedback.get("recommendations", [])
    encouragement = structured_feedback.get("encouragement", "")
    attempt_number = structured_feedback.get("attempt_number", 1)
    
    # Build user prompt based on whether there are errors or not
    has_errors = main_issues and len(main_issues) > 0
    
    user_prompt = f"""ã€ç·´ç¿’ #{attempt_number} ã®ç™ºéŸ³è©•ä¾¡ã€‘

å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ: "{reference_text}"
ç™ºéŸ³ã‚¹ã‚³ã‚¢: {overall_score:.0f}/100

"""
    
    if has_errors:
        # If there are errors, focus on them
        user_prompt += "**ç™ºéŸ³ãƒŸã‚¹:**\n"
        for issue in main_issues[:3]:
            user_prompt += f"- {issue}\n"
        user_prompt += "\nã“ã‚Œã‚‰ã®ãƒŸã‚¹ãŒãªãœèµ·ããŸã®ã‹ã€ã©ã†ç›´ã›ã°ã„ã„ã‹ã€å„ªã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"
    else:
        # If no errors, praise and focus on prosody
        user_prompt += "**ç´ æ™´ã‚‰ã—ã„ï¼ç™ºéŸ³ãƒŸã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚**\n\n"
        if improvements:
            user_prompt += "å‰å›ã‹ã‚‰ã®æ”¹å–„ç‚¹:\n"
            for improvement in improvements[:2]:
                user_prompt += f"- {improvement}\n"
            user_prompt += "\n"
        user_prompt += "ä»Šå›ã®ç™ºéŸ³ã®è‰¯ã‹ã£ãŸç‚¹ã‚’å…·ä½“çš„ã«è¤’ã‚ã¦ãã ã•ã„ã€‚\n"
        user_prompt += "å¯èƒ½ã§ã‚ã‚Œã°ã€éŸ»å¾‹ï¼ˆã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒªã‚ºãƒ ã€å¼·å‹¢ãªã©ï¼‰ã®é¢ã§ã•ã‚‰ã«è‰¯ãã§ãã‚‹ç‚¹ãŒã‚ã‚Œã°å„ªã—ãææ¡ˆã—ã¦ãã ã•ã„ã€‚"
    
    user_prompt += "\n\næ¸©ã‹ãåŠ±ã¾ã—ãªãŒã‚‰ã€æ¬¡ã®ç·´ç¿’ã¸ã®ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é«˜ã‚ã¦ãã ã•ã„ã€‚"
    
    # Try to get model/deployment name from secrets, with fallbacks
    try:
        # First try to get from secrets
        model_name = st.secrets["AzureGPT"].get("DEPLOYMENT_NAME", None)
        if not model_name:
            # Fallback to common names
            model_name = "gpt-5-mini"  # Your deployment name
    except:
        model_name = "gpt-5-mini"  # Default fallback
    
    try:
        # Note: gpt-5-mini is a reasoning model
        # Not setting max_completion_tokens to use the default limit
        
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
            # No max_completion_tokens - use default
        )
        
        content = response.choices[0].message.content
        
        # Return content or fallback message
        if content and len(content.strip()) > 0:
            return content
        else:
            # If empty, return a friendly message
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚AIãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ç”Ÿæˆã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
    except Exception as e:
        # If AI fails, show the error and return structured feedback
        st.warning(f"âš ï¸ AIç”Ÿæˆã«å¤±æ•—: {type(e).__name__}: {str(e)}")
        st.info(f"ğŸ’¡ ãƒ¢ãƒ‡ãƒ« '{model_name}' ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚æ§‹é€ åŒ–ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
        
        feedback_lines = [
            f"## ç™ºéŸ³è©•ä¾¡ - ç·´ç¿’ #{attempt_number}",
            f"",
            f"**ã‚¹ã‚³ã‚¢:** {overall_score:.0f}/100 ({score_label})",
            f""
        ]
        
        if improvements:
            feedback_lines.append("### âœ… æ”¹å–„ã•ã‚ŒãŸãƒã‚¤ãƒ³ãƒˆ")
            for improvement in improvements:
                feedback_lines.append(f"- {improvement}")
            feedback_lines.append("")
        
        if main_issues:
            feedback_lines.append("### ğŸ¯ æ”¹å–„ãŒå¿…è¦ãªãƒã‚¤ãƒ³ãƒˆ")
            for issue in main_issues:
                feedback_lines.append(f"- {issue}")
            feedback_lines.append("")
        
        if recommendations:
            feedback_lines.append("### ğŸ’¡ æ¨å¥¨ã•ã‚Œã‚‹ç·´ç¿’æ–¹æ³•")
            for rec in recommendations:
                feedback_lines.append(f"- {rec}")
            feedback_lines.append("")
        
        feedback_lines.append(f"**{encouragement}**")
        
        return "\n".join(feedback_lines)


tabs = st.tabs(["ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°", "ç™ºéŸ³ã®å¯è¦–åŒ–", "ã¾ã¨ã‚"])

with tabs[0]:
    cols = st.columns([0.6, 0.4])

    with cols[0]:
        # Load video and text
        load_video(user, lesson)
        reference_text = load_text(user, lesson)

        with st.form("audio_input"):
            audio_bytes_io = st.audio_input(f"éŸ³å£°ã‚’éŒ²éŸ³ã—ã¦ã¿ã‚ˆã†", sample_rate=48000)

            submitted = st.form_submit_button("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ã‚‚ã‚‰ãŠã†ï¼")
            if submitted:
                # Get pronunciation assessment
                st.session_state.practice_times += 1
                audio_file_path = f"asset/{user}/history/{lesson}-{st.session_state.practice_times}.wav"
                
                # TESTING: Use saved JSON for now
                # save_audio_to_file(audio_bytes_io, filename=audio_file_path)
                # pronunciation_assessment_result = get_pronunciation_assessment(
                #     user, st.session_state.pronunciation_config, reference_text, audio_file_path
                # )
                
                import json
                with open("asset/1/history/test1.json", "r", encoding="utf-8") as f:
                    pronunciation_assessment_result = json.load(f)
                
                # Save assessment
                assessment_path = f"asset/{user}/history/{lesson}-{st.session_state.practice_times}.json"
                save_pronunciation_assessment(pronunciation_assessment_result, assessment_path)

                # ========== CAPT PIPELINE INTEGRATION ==========
                with st.spinner("AI ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆä¸­..."):
                    # Use the simplified process_attempt method
                    # It handles guidance card creation, attempt summaries, and feedback automatically
                    structured_feedback = st.session_state.capt_pipeline.get_structured_feedback(
                        pronunciation_assessment_result,
                        attempt_number=st.session_state.practice_times
                    )
                    
                    # Store guidance card reference for UI display
                    if st.session_state.practice_times == 1:
                        st.session_state.guidance_card = st.session_state.capt_pipeline.guidance_card
                        st.success("âœ… åˆå›åˆ†æå®Œäº†ï¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚«ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
                    else:
                        st.success(f"âœ… å·®åˆ†åˆ†æå®Œäº†ï¼ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: ç´„90%å‰Šæ¸›ï¼‰")
                    
                    # Generate AI feedback using OpenAI with structured data
                    with st.spinner("ğŸ¤– AI ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆä¸­..."):
                        st.session_state.ai_feedback_text = generate_ai_feedback_from_structured(
                            st.session_state.openai_client,
                            structured_feedback,
                            reference_text
                        )
                    
                    # Simple success message
                    if st.session_state.ai_feedback_text:
                        st.success("âœ… AI ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆå®Œäº†ï¼")
                # ========== END CAPT INTEGRATION ==========

                # Create radar chart for visualization
                radar_chart = create_radar_chart(pronunciation_assessment_result)
                st.session_state["feedback"]["radar_chart"] = radar_chart

    with cols[1]:
        # Radar Chart Container
        with st.container(height=400, horizontal_alignment="center", vertical_alignment="center"):
            if st.session_state["feedback"]["radar_chart"] is None:
                st.html("<h1 style='text-align: center;'>ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ</h1>")
            else:
                st.pyplot(st.session_state["feedback"]["radar_chart"], width=450)

        # ========== SIMPLIFIED AI FEEDBACK CONTAINER ==========
        with st.container(height=450, horizontal_alignment="center", vertical_alignment="center"):
            if st.session_state.ai_feedback_text is None:
                st.html("<h2 style='text-align: center;'>AIãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯</h2>")
                st.info("ğŸ¤ éŸ³å£°ã‚’éŒ²éŸ³ã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å—ã‘å–ã‚Šã¾ã—ã‚‡ã†ï¼")
            else:
                # Header
                st.markdown(f"### ğŸ¯ AIãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ (ç·´ç¿’ #{st.session_state.practice_times})")
                st.markdown("---")
                
                # Main AI feedback text only
                st.markdown(st.session_state.ai_feedback_text)
        # ========== END SIMPLIFIED FEEDBACK CONTAINER ==========

with tabs[1]:
    st.header("ç™ºéŸ³ã®å¯è¦–åŒ–")
    
    region_colors = RegionColorOptions(
        interactive_region_color="rgba(160, 211, 251, 0.4)",
        start_to_end_mask_region_color="rgba(160, 211, 251, 0.3)"
    )
    custom_regions = [
        CustomizedRegion(start=6, end=6.5, color="#00b89466"),
        CustomizedRegion(start=7, end=8, color="rgba(255, 255, 255, 0.6)")
    ]
    audix("asset/1/history/9.wav", key="target")
    result = audix(
        "asset/1/history/9.wav",
        start_time=0.5,
        end_time=5.5,
        mask_start_to_end=True,
        region_color_options=region_colors,
        customized_regions=custom_regions,
        key="user"
    )
    with st.container(height=300, horizontal_alignment="center", vertical_alignment="center"):
        st.html("<h2 style='text-align: center;'>éŸ³ç¯€çš„ãªç™ºéŸ³ã®çµ±è¨ˆè¡¨</h2>")

with tabs[2]:
    st.header("ã¾ã¨ã‚")
    inner_cols3 = st.columns(2)
    
    with inner_cols3[0]:
        with st.container(height=400, horizontal_alignment="center", vertical_alignment="center"):
            st.html("<h2 style='text-align: center;'>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿1</h2>")
        with st.container(height=400, horizontal_alignment="center", vertical_alignment="center"):
            st.html("<h2 style='text-align: center;'>å††ã‚°ãƒ©ãƒ•1</h2>")

    with inner_cols3[1]:
        with st.container(height=400, horizontal_alignment="center", vertical_alignment="center"):
            st.html("<h2 style='text-align: center;'>æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿2</h2>")
        with st.container(
            height=400, horizontal_alignment="center", vertical_alignment="center"
        ):
            st.html("<h2 style='text-align: center;'>å††ã‚°ãƒ©ãƒ•2</h2>")
